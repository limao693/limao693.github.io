{"meta":{"title":"万物并作，吾以观复","subtitle":null,"description":"私人空间","author":"Charles li","url":"http://yoursite.com"},"pages":[{"title":"About","date":"2022-02-16T23:06:11.380Z","updated":"2022-02-16T23:06:11.380Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""},{"title":"Project","date":"2022-02-16T23:06:11.380Z","updated":"2022-02-16T23:06:11.380Z","comments":true,"path":"project/index.html","permalink":"http://yoursite.com/project/index.html","excerpt":"","text":""},{"title":"Tags","date":"2022-02-16T23:06:11.380Z","updated":"2022-02-16T23:06:11.380Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"python协程gevent","slug":"yuque/python协程gevent","date":"2020-05-26T07:00:58.000Z","updated":"2022-02-16T23:07:27.742Z","comments":true,"path":"2020/05/26/yuque/python协程gevent/","link":"","permalink":"http://yoursite.com/2020/05/26/yuque/python协程gevent/","excerpt":"","text":"协程由于 python 解释器使用全局解释器锁（Global Interpreter Lock，GIL），设计锁是为了同步线程。能够确保任一时刻，仅有一个线程真正的在工作。所以为了能够实现多线程工作，引出了协程（Coroutine）。协程作为一种计算机程序组件，能够允许挂起和恢复执行来概括非抢占式多任务处理的子流程。协程非常适合于实现熟悉的程序组件，例如协作任务，异常，事件循环，迭代器，无线列表和管道。 Python，通过使用 Generator 实现了协程，协程可以理解为遵守某种规则的生成器，因此包含 yield 关键字。但是，在协程内， yield 方式转移执行权。 在 python 中协程的库主要有 greenlet, stackless, gevent, eventlet 等实现。 协程的特点 调度由用户控制 一个线程或者进程可以有多个协程 每个线程（进程）循环安装指定的顺序完成不同的任务（可以阻塞后，执行下一个任务；当恢复时，再继续执行任务） 协程需要保证是非依赖的，非阻塞的。原因是，协程可以随时中断，切换到其他任务 协程一般采用异步通信。 协程的使用使用协程，完成一个生产者、消费者模型 1234567891011121314151617181920def consumer(): r = '' while True: n = yield r if n is None: return print('[CONSUMER] Consumer %s...' %n) r = '200 OK' def produce(c): c.send(None) n = 0 while n &lt; 5: n = n + 1 print('[PRODUCE] Producing %s...' % n) r = c.send(n) c.close()c = consumer()produce(c) 任务队列 12345678910111213141516171819from collections import dequedef countdown(n): while n &gt; 0: yield n n -= 1tasks = deque()tasks.extend([countdown(10), countdown(5), countdown(20)])def run(): while tasks: task = tasks.popleft() try: x = next(task) print(x) tasks.append(task) except StopIteration: print('Task') Geventgevent 是协程 web 并发连接库，此协程库基于 greenlet gevent 特点gevent 原理gevent 示例","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"},{"name":"协程","slug":"协程","permalink":"http://yoursite.com/tags/协程/"}]},{"title":"Docker基础知识--Cgroup","slug":"yuque/Docker基础知识--Cgroup","date":"2020-01-08T06:24:05.000Z","updated":"2022-02-16T23:07:27.802Z","comments":true,"path":"2020/01/08/yuque/Docker基础知识--Cgroup/","link":"","permalink":"http://yoursite.com/2020/01/08/yuque/Docker基础知识--Cgroup/","excerpt":"","text":"前言上篇中，已经讨论了 Docker 作为虚拟化技术，所采用的基础技术 namespace，以此来进行资源隔离。同时，我们也想到了另外一个问题，在资源隔离后，实际上仍然是占用 host 的资源，如何保证资源（CPU、内存和 IO）被有效的控制，为此 Docker 引入另外一项关键技术—Control group ( Cgroup)。 何为 Cgroup在 Linux 系统中，内核本身的调度和管理并不对进程和线程进行区分，只会根据clone创建时传入的值不同，来区分进程和线程。Cgroup 全称为 control grup ，由 06 年合入 linux 内核，就是把任务放在一个组里进行控制。我们给出官方的定义：cgroup 是 Linux 内核提供的一种机制，这种机制可以根据需要把一系列的系统任务和对应的子任务整合（或者分割）到资源划分等级的不同组内。从而对系统的资源，提供了一个完整的控制框架。 Cgroup 的四个特点 cgroup 的 API 以仿文件系统的方式实现，用户态的进程可以通过管理文件的方式完成组织管理 cgroup 的组织管理，最小颗粒度为线程级别，此外用户有权限创建和销毁 cgroup，从而实现资源再分配 所有的资源管理的功能都以子系统的方式实现，保证 API 接口的统一性 所有子系统的资源组与父任务相同 所有我们，可以认为，cgroup 是附加在程序上的钩子（hook），通过程序运行时，对资源进行调度触发钩子达到资源监控和控制。 Cgroup 原理在介绍 Cgroup 原理之前，我们先熟悉两个概念：组织结构与基本规则、子系统。 Cgroup 的组织结构传统的 Unix 体系中，首先启动 init 的进程作为 root 进程，再由 init 进程扩展创建子系统作为子节点，而每个子节点又重复此方式，往复循序。而 Cgroup 也创建类似树状结构，子节点继承自父节点。与传统 init 方式不同，cgroup 运行多个父节点，即 cgroup 子节点对应的父节点可以有多个。在 Docker 中每个子系统独自构成一个层级，便于管理。 Cgroup 的组织规则 同一个层级可以附加多个子系统 一个子系统可以附加到多个层级，但前提是，关联的层级仅能与此子系统相连，即当且仅当目标层级只有唯一一个子系统时。 系统每次创建一个新的层级，该系统上所有任务默认加入这个新建层级的初始化层级，即 root cgroup 。任务在同一层级，仅能有一份。 任务在 fork/clone 自身时，创建的子任务默认与原任务在同一层级（cgroup）中，但子任务运行被移动到不同个的 cgroup 中。 子系统介绍blkio – 这个子系统为块设备设定输入/输出限制，比如物理设备（磁盘，固态硬盘，USB 等等）。cpu – 这个子系统使用调度程序提供对 CPU 的 cgroup 任务访问。cpuacct – 这个子系统自动生成 cgroup 中任务所使用的 CPU 报告。cpuset – 这个子系统为 cgroup 中的任务分配独立 CPU（在多核系统）和内存节点。devices – 这个子系统可允许或者拒绝 cgroup 中的任务访问设备。freezer – 这个子系统挂起或者恢复 cgroup 中的任务。memory – 这个子系统设定 cgroup 中任务使用的内存限制，并自动生成由那些任务使用的内存资源报告。net_cls – 这个子系统使用等级识别符（classid）标记网络数据包，可允许 Linux 流量控制程序（tc）识别从具体 cgroup 中生成的数据包。ns – 名称空间子系统。","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"},{"name":"Cgroup","slug":"Cgroup","permalink":"http://yoursite.com/tags/Cgroup/"}]},{"title":"Docker基础原理--namespace（pid与network）","slug":"yuque/Docker基础原理--namespace（pid与network）","date":"2019-12-18T05:10:11.000Z","updated":"2022-02-16T23:07:27.874Z","comments":true,"path":"2019/12/18/yuque/Docker基础原理--namespace（pid与network）/","link":"","permalink":"http://yoursite.com/2019/12/18/yuque/Docker基础原理--namespace（pid与network）/","excerpt":"","text":"前言在计算机系统中为了标识不同的作用域，而引入 namespace（命名空间）的概念。namespace 作用在内核级别的隔离，可以有效阻止不同进程间的通信。在日常的 Linux 系统中，通常服务在启动后的进程，彼此之间不会特意进行隔离，例如 Web 服务器中，Nginx、MySQL 和 rides，三个服务之间亦可访问任意主机中的文件。如果任一服务不慎被攻破，会被入侵其他的服务。因而 namespace 的引入，可以有效防止不同服务或进程间的相互干扰。Docker 使用 Linux 的 namespace 技术，对实现的容器，做了多种层次的隔离。Docker 所使用的七种 namespace 如下：CLONE_NEWCGROUP、CLONE_NEWIPC、CLONE_NEWNET、CLONE_NEWNS、CLONE_NEWPID、CLONE_NEWUSER 和 CLONE_NEWUTS。通过在进程中添加如上属性，便能保证与宿主机的对应资源进行有效隔离。 进程在分时操作系统中，进程作为程序的基本执行单元，也是资源（CPU、内存）分配的基本单位。当服务、程序启动后，操作系统为其开辟一个进程，并为它分配资源，除等待 CPU 资源，其他资源以及就绪，此时进入到就绪态。当时间片分割的进程，在高优先级调度队列被调度后，进程开始运行。进程的三种状态：就绪态、运行态和阻断状态。Linux 的进程，可以通过ps -elf命令进行查看，下图为 Centos 系统的截图： 123456ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 9月24 ? 00:01:32 /usr/lib/systemd/systemd --switched-root --system --deserialize 22root 2 0 0 9月24 ? 00:00:00 [kthreadd]root 3 2 0 9月24 ? 00:00:00 [rcu_gp]root 4 2 0 9月24 ? 00:00:00 [rcu_par_gp] 当前机器中，运行着很多进程，但都以一个 1 号进程开始，即 pid=1 的进程 init。此进程作为所以后续进程的父进程。例如 pid=39 的进程，其父进程（PPID=1）。 pid=0 号进程 idle，其前身是系统创建的第一个进程，也是唯一一个没有通过 fork 或者 kernel_thread 产生的进程。完成加载系统后，演变为进程调度、交换 pid=1 号进程 init，负责执行内核的初始化工作和系统配置。 pid=2 的 kthreadd 进程，始终运行在内核空间, 负责所有内核线程的调度和管理。 Docker 容器内进程 PID以 RabbitMQ 容器为例，通过如下命令进入 rabbitmq 容器后，使用 ps -ef 命令，再次查看容器内的进程，可以与上面的系统进程进行比较。 12345[root@li1559-144 ~]# docker exec -it 28b6862b34a9 bashroot@28b6862b34a9:/# ps -efUID PID PPID C STIME TTY TIME CMDrabbitmq 1 0 0 16:07 ? 00:00:00 /bin/sh /opt/rabbitmq/sbin/rabbitmq-serverrabbitmq 138 1 0 16:07 ? 00:00:00 /usr/local/lib/erlang/erts-10.4.4/bin/epmd -daemon 当前容器内，PID=1 的进程为 rabbitmq-server，与 Linux 的宿主机的 PID=1 的 init 进程不同。此时，容器内的进程以及与宿主机完全隔离。宿主机与 rabbitmq 容器之间的进程对应关系，如下所示（摘自@Draveness）：因此容器内的进程，对宿主机的进程一无所知。实现了内核级的隔离。 Docker 在使用上述隔离技术，进行资源的隔离。当我们每次运行 docker run 或者 docker start 时，都会调用以下代码块（moby:https://github.com/moby），创建资源隔离的容器： 12345678910111213141516171819202122func (daemon *Daemon) createSpec(c *container.Container) (retSpec *specs.Spec, err error) &#123; var ( opts []coci.SpecOpts s = oci.DefaultSpec() ) opts = append(opts, WithCommonOptions(daemon, c), WithCgroups(daemon, c), //添加Cgroups，进行资源限制 WithResources(c), WithSysctls(c), WithDevices(daemon, c), WithUser(c), WithRlimits(daemon, c), WithNamespaces(daemon, c), //添加namespace，进行资源隔离 WithCapabilities(c), WithSeccomp(daemon, c), WithMounts(daemon, c), //添加挂载点 WithLibnetwork(daemon, c), WithApparmor(c), WithSelinux(c), WithOOMScore(&amp;c.HostConfig.OomScoreAdj), ) 在 WithNamespaces(daemon, c) 函数中，会设置 user network ipc pid uts 以及 cgroup ，需要主要的是 moby 项目中，CLONE_NEWNS即文件系统被单独在 WithMounts(daemon, c) 函数中设置。 1234567891011121314151617181920212223242526272829303132func WithNamespaces(daemon *Daemon, c *container.Container) coci.SpecOpts &#123; return func(ctx context.Context, _ coci.Client, _ *containers.Container, s *coci.Spec) error &#123; userNS := false // user // network // ipc //pid if c.HostConfig.PidMode.IsContainer() &#123; ns := specs.LinuxNamespace&#123;Type: \"pid\"&#125; pc, err := daemon.getPidContainer(c) if err != nil &#123; return err &#125; ns.Path = fmt.Sprintf(\"/proc/%d/ns/pid\", pc.State.GetPID()) setNamespace(s, ns) if userNS &#123; // to share a PID namespace, they must also share a user namespace nsUser := specs.LinuxNamespace&#123;Type: \"user\"&#125; nsUser.Path = fmt.Sprintf(\"/proc/%d/ns/user\", pc.State.GetPID()) setNamespace(s, nsUser) &#125; &#125; else if c.HostConfig.PidMode.IsHost() &#123; oci.RemoveNamespace(s, specs.LinuxNamespaceType(\"pid\")) &#125; else &#123; ns := specs.LinuxNamespace&#123;Type: \"pid\"&#125; setNamespace(s, ns) &#125; // uts //cgroup return nil &#125;&#125; Docker 容器内网络 NetworkLinux 的 namespace 主要隔离的对象有：网络设备、IPv4 和 IPv6 协议栈、路由表、防火墙、/proc/net 目录、/sys/class/net 目录、套接字（socket）等。Linux 中，物理网络设备默认放置在 root namespace 中。Docker 中为实现网络的独立，又能与宿主机进行通信，为此创建 veth pair。将其中的一段放置在容器内通常，通常命名为 eth0，另外一段挂载为物理网络设备上所在的 namespace 中。实际中，docker 在 docker daemon 启动后会创建 docker0 网桥，docker 容器的网络对另一端通常绑定在 docker0 网桥。现行的 docker 项目中，自 docker&gt;1.9 之后，已经将网络单独剥离为 libnetwork 库，自此网络模式也被抽象为同样接口。docker 现有的网络模式有五种： bridge驱动 、 host驱动 、 overlay驱动 、 remote驱动 、 null驱动 。接下来，我们手动创建 veth pair ，用于模仿 docker 网桥模式。docker0（lxcbr0）网桥作为二层交换网桥，之所以在上面配置 IP，可以认为内部有一个可以用于配置 IP 信息的网卡接口。Docker 在桥接模式下，docker0 网桥用于连接容器上的默认网关而存在，并且可以保证在必要时候与宿主机进行网络隔离。 12345678910111213141516171819202122232425262728293031323334353637383940## 首先，我们先增加一个网桥lxcbr0，模仿docker0brctl addbr lxcbr0brctl stp lxcbr0 offifconfig lxcbr0 192.168.10.1/24 up #为网桥设置IP地址## 接下来，我们要创建一个network namespace - ns1# 增加一个namesapce 命令为 ns1 （使用ip netns add命令）ip netns add ns1# 激活namespace中的loopback，即127.0.0.1（使用ip netns exec ns1来操作ns1中的命令）ip netns exec ns1 ip link set dev lo up## 然后，我们需要增加一对虚拟网卡# 增加一个pair虚拟网卡，注意其中的veth类型，其中一个网卡要按进容器中ip link add veth-ns1 type veth peer name lxcbr0.1# 把 veth-ns1 按到namespace ns1中，这样容器中就会有一个新的网卡了ip link set veth-ns1 netns ns1# 把容器里的 veth-ns1改名为 eth0 （容器外会冲突，容器内就不会了）ip netns exec ns1 ip link set dev veth-ns1 name eth0# 为容器中的网卡分配一个IP地址，并激活它ip netns exec ns1 ifconfig eth0 192.168.10.11/24 up# 上面我们把veth-ns1这个网卡按到了容器中，然后我们要把lxcbr0.1添加上网桥上brctl addif lxcbr0 lxcbr0.1# 为容器增加一个路由规则，让容器可以访问外面的网络ip netns exec ns1 ip route add default via 192.168.10.1# 在/etc/netns下创建network namespce名称为ns1的目录，# 然后为这个namespace设置resolv.conf，这样，容器内就可以访问域名了mkdir -p /etc/netns/ns1echo \"nameserver 8.8.8.8\" &gt; /etc/netns/ns1/resolv.conf## 引自：https://coolshell.cn/articles/17029.html 详细的 Linux 的 ip 命令，可以参见：https://wangchujiang.com/linux-command/c/ip.html","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"},{"name":"namespace","slug":"namespace","permalink":"http://yoursite.com/tags/namespace/"}]},{"title":"Go语言的sync模块","slug":"yuque/Go语言的sync模块","date":"2019-09-03T08:11:27.000Z","updated":"2022-02-16T23:07:27.902Z","comments":true,"path":"2019/09/03/yuque/Go语言的sync模块/","link":"","permalink":"http://yoursite.com/2019/09/03/yuque/Go语言的sync模块/","excerpt":"","text":"在程序进行并发操作时，当不同线程使用同一变量时，由于无法判断是否有其他线程在进行写操作，即线程之间出现竞争（资源竞争），因此引入锁机制。Go 语言使用 sync 模块保证线程在操作变量的互斥性。当变量被一个线程操作时，先使用 sync 将变量上锁，退出操作时，解锁。sync 模块提供基础的同步原语，例如互斥锁（mutual exclusion locks）。除了Once和WaitGroup类型，大部分的 sync 模块的方法使用在低级库。更高级别的同步方式是使用通道（channel）和通信（communication）。本文主要介绍 sync 包内，一些锁的概念及使用方式。 sync 包基础sync 包围绕 sync.Locker 进行，其中 interface 为： 1234type Locker interface &#123; Lock() Unlock()&#125; 基本锁 Mutexsync.Mutex 是互斥锁，相当于在变量入口处确保只有一个线程能够操作变量。假如 num 是一个需要锁处理的存储在共享内存中的变量。通过 Mutex 处理加锁与解锁。具体例子如下： 1234567891011121314import \"sync\"type num struct &#123; mutex sync.Mutex nu int&#125;//flash numfunc update(num *num) &#123; num.mutex.Lock() //update num num.nu = // new value num.mutex.Unlock() //任何 goroutine 都可用开锁&#125; 单写多读锁 RWMutex顾名思义，允许多个线程进行读取，但仅允许单个线程进行写操作。通过 Rlock() 允许同一时间多个线程读操作；如果使用 Lock() ，则 RWMutex 变成 Mutex。RWMutex 的方法有： 12345func (rw *RWMutex) Lock()func (rw *RWMutex) RLock()func (rw *RWMutex) RLocker() Lockerfunc (rw *RWMutex) RUnlock()func (rw *RWMutex) Unlock() 对应的策略有： 允许存在多个读锁，但只能有一把写锁； 当写锁未被释放时或此时有正被等待的写锁，读锁不可用； 只有当全部读锁结束，写锁才可用； sync.Oncesync.Once 可以保证被调用的 func，只调用一次。Once 数据结构为： 1234567891011121314151617type Once struct &#123; m Mutex done uint32&#125;func (o *Once) Do(f func()) &#123; if atomic.LoadUint32(&amp;o.done) == 1 &#123; return &#125; // Slow-path. o.m.Lock() defer o.m.Unlock() if o.done == 0 &#123; defer atomic.StoreUint32(&amp;o.done, 1) f() &#125;&#125; 由 once.Do 可以看出，当once.Do(f)被调用多次时，只有第一次会调用功能函数 f。其核心思想是，使用原子计数记录被执行的次数。其中的锁机制，仍旧使用Mutex. 官方给出的一个示例，及对应的执行结果如下： 123456789101112131415161718192021222324252627//官方例子package mainimport ( \"fmt\" \"sync\")func main() &#123; var once sync.Once onceBody := func() &#123; fmt.Println(\"Only once\") &#125; done := make(chan bool) for i := 0; i &lt; 10; i++ &#123; go func() &#123; once.Do(onceBody) done &lt;- true &#125;() &#125; for i := 0; i &lt; 10; i++ &#123; &lt;-done &#125;&#125;// 打印输出为：Only once sync.WaitGroup A WaitGroup waits for a collection of goroutines to finish. The main goroutine calls Add to set the number of goroutines to wait for. Then each of the goroutines runs and calls Done when finished. At the same time, Wait can be used to block until all goroutines have finished. WaitGroup 可以保证集合内所有 goroutines 协程完成后，主进程再执行其他动作。避免协程未执行完，主进程就退出或执行其他影响协程的动作。 WaitGroup 操作方式执行协程前使用 func (WaitGroup) Add 添加等待计数器。协程完成一次，执行 func (WaitGroup) Done，在 waitgroup 计数器中减少一个，直至计数器为零，释放锁。 使用举例123456789101112131415161718192021222324252627282930313233package mainimport ( \"sync\")type httpPkg struct&#123;&#125;func (httpPkg) Get(url string) &#123;&#125;var http httpPkgfunc main() &#123; var wg sync.WaitGroup var urls = []string&#123; \"http://www.golang.org/\", \"http://www.google.com/\", \"http://www.somestupidname.com/\", &#125; for _, url := range urls &#123; // Increment the WaitGroup counter. wg.Add(1) // Launch a goroutine to fetch the URL. go func(url string) &#123; // Decrement the counter when the goroutine completes. defer wg.Done() // Fetch the URL. http.Get(url) &#125;(url) &#125; // Wait for all HTTP fetches to complete. wg.Wait()&#125;","categories":[],"tags":[]},{"title":"RPC调用","slug":"yuque/RPC调用","date":"2019-06-02T19:41:05.000Z","updated":"2022-02-16T23:07:27.910Z","comments":true,"path":"2019/06/02/yuque/RPC调用/","link":"","permalink":"http://yoursite.com/2019/06/02/yuque/RPC调用/","excerpt":"","text":"知识点Targetan RPC Server’s target: topic and server is required; exchange is optionalan RPC endpoint’s target: namespace and version is optionalan RPC client sending a message: topic is required, all other attributes optionala Notification Server’s target: topic is required, exchange is optional; all other attributes ignoreda Notifier’s target: topic is required, exchange is optional; all other attributes ignored","categories":[],"tags":[{"name":"rpc","slug":"rpc","permalink":"http://yoursite.com/tags/rpc/"},{"name":"HTTP","slug":"HTTP","permalink":"http://yoursite.com/tags/HTTP/"}]},{"title":"usb接口键盘转PS2接口","slug":"yuque/usb接口键盘转PS2接口","date":"2019-03-03T07:09:24.000Z","updated":"2022-02-16T23:07:27.930Z","comments":true,"path":"2019/03/03/yuque/usb接口键盘转PS2接口/","link":"","permalink":"http://yoursite.com/2019/03/03/yuque/usb接口键盘转PS2接口/","excerpt":"","text":"用途公司或者主机设备没有 usb 接口，只能使用 PS/2 接口键盘。此程序完成 USB HID keyboard 协议向 PS/2 的 scan code set 2 转换。 需要的硬件设备 arduino uno R3 USB Host Shield 使用说明 默认 PS/2 接口的的四条数据线中，数据段和时钟段连接在 Arduino 的（4，2）。 使用 IDE 刷入 usb2ps2.ino 即可 源代码地址https://github.com/limao693/usb2ps2 参考文件 USB HID to PS/2 Scan Code Translation Table PS2 键盘接口说明 模拟 PS/2 键盘 Application scenarioThe company or host device does not have a usb interface and can only use the PS/2 interface keyboard. This program completes the conversion of the USB HID keyboard protocol to the PS/2 scan code set 2. Required hardware devices Arduino uno R3 USB Host Shield Instructions for use Among the four data lines of the default PS/2 interface, the data segment and the clock segment are connected to the Arduino (4, 2). Use the IDE to brush usb2ps2.ino Source codehttps://github.com/limao693/usb2ps2 reference document USB HID to PS/2 Scan Code Translation Table PS2 Keyboard Interface Description Analog PS/2 Keyboard","categories":[],"tags":[{"name":"Arduino","slug":"Arduino","permalink":"http://yoursite.com/tags/Arduino/"},{"name":"硬件","slug":"硬件","permalink":"http://yoursite.com/tags/硬件/"}]},{"title":"Kafka - - 快速入门指南","slug":"yuque/Kafka - - 快速入门指南","date":"2019-02-01T02:41:49.000Z","updated":"2022-02-16T23:07:28.114Z","comments":true,"path":"2019/02/01/yuque/Kafka - - 快速入门指南/","link":"","permalink":"http://yoursite.com/2019/02/01/yuque/Kafka - - 快速入门指南/","excerpt":"","text":"简介Kafka 起源于 2011 年 LikedIn 的开源项目，并不断发展。如今它是一个完整的平台，允许您冗余地存储巨大的数据量，拥有一个具有巨大吞吐量（数百万/秒）的消息总线，同时可实现实时流处理。 Kafka 具备的特性有：分布式、水平扩展、高容错和日志管理。接下来将逐个介绍特性。 分布式分布式系统的显著特点是将任务分配到多个机器处理，即组成的集群对外暴露的形式仍为单一节点。Kafka 的分布特性在于在不同节点（代理）存储、接收和发送消息。 分布式系统的介绍可以参阅：A Thorough Introduction to Distributed Systems 分布式系统的优势主要体现在：高可扩展性、容错性。 水平扩展首先解释、定义垂直扩展。例如，数据库服务器过载，最直接的解决办法是添加资源（CPU、RAM 和 SSD）。垂直扩展的两大缺点是： 现有的硬件限制了扩展能力。例如不能无线提高 CPU 核数。 通常需要停机时间。 水平扩展通过投入更多的机器来解决上述问题。添加新机器不需要停机，也没有机器数量的限制。它的缺点是，并非所有系统支持水平弹性伸缩，因为在设计之初没有考虑工作在集群环境下，同时设计也更为复杂。 高容错性非分布式系统的一大安全隐患是单点故障（single point of failure, SPOF）。例如单实例数据库宕机，会产生无法估量的损失。分布式系统被设计为可配置的方式，来处理故障。例如，在 5 节点的 Kafka 集群中，即使其中两个节点关闭，系统仍然继续工作。需要注意的地方是，系统容错率与系统性能成反比，即容错率越高，性能越低。 日志采集日志采集（事务日志）仅支持附加的持久有序数据结构，即无法修改或删除已有的记录。读取顺序从左向右如图所示。在某种程度来讲，Kafka 的数据结构就是如此简单。这也是 Kafka 的核心，因为它提供了有序数据，而有序的数据结构提供了确定性的处理方式。Kafka 实际上将所有消息存储在磁盘上，并且对他们进行排序，最大化利用硬盘顺序读取速度快的优势。 读取和写入是整数级别 O(1)（知道记录 ID），与磁盘上的其他数据结构的操作复杂度为 O(log N)相比，有巨大优势。 读取和写入是分离的。写入时不会锁定读取，反之亦然（与平衡树相反）。 这两点使得 Kafka 的性能有巨大优势，因为系统性能与数据量是分离的。Kafka 无论处理 100KB 的数据量还是 100TB 的数据量，性能都是一样的。 工作原理向 Kafka 节点（broker）发送消息（记录）的称为程序（生产者），发送给其他程序处理消息的称为消费者。产生的消息存储在一个topic内，消费者订阅向 topic 订阅后，可以接收到对应 topic 的新消息。随着 topic 变得越来越大，为保持性能和可伸缩性将 topic 拆分成更小的分区。（例如，需要存储用户的登录名，可以将用户名按照首字母进行拆分存储在不同的分区）Kafka 保证分区内的所有消息都按照它们进入的顺序排序。区分特定消息的方式是通过其偏移量 ，可将其视为普通数组索引，在一个分区的序列号随着新消息进入而递增。Kafka 遵循愚蠢的 broker 和聪明的消费者原则。意思是，Kafka 不会跟踪哪一条记录已经被消费者读取并删除，而是将他们按照一定的时间（例如一天）或约定某个阈值大小来存储。消费者自身从 Kafka 的 broker主动拉取新信息，并告知他们想要读取的片段。这意味着 broker 允许消费者按照自身的需要进行增加或减小偏移量，因此具有重新读取和重新处理信息的能力。需要注意的是，消费者其实是消费者群组，包含一个或多个消费进程。为避免两个进程重复读取相同的信息，每个分区仅与每个组的一个消费者进程相关联。 数据持久化Kafka 将所有记录存储在磁盘，并且不会在 RAM 中保存任何记录。你可能会惊讶于以怎样高效的方式来做明智的选择。这背后有很多值得学习的优化知识： Kafka 有将成组的消息合并的协议。允许网络请求将消息组合在一起减少网络开销，因此服务器一次性保留大量消息，消费者一次获取巨大的线性消息块。 磁盘顺序 I/O 读取速度很快。现代磁盘速度慢的原因是由于大量的磁盘寻道，但是在大型线性读写操作时不是问题。 线性操作由 OS 进一步优化，通过预读取（预读取多倍数据块）和后写入（将小的逻辑写入操作合并为组后再进行物理写入操作）技术。 现代 OS 利用 RAM 模拟磁盘缓存，这种技术成为 pagecache。 由于 kafka 在整个流程（生产者—&gt;代理—&gt;消费者）中以未经修改的标准化二进制格式存储消息，因此它可以使用零拷贝（zero-copy）优化。操作系统将数据从 pagecache 直接复制到套接字，有效地完全绕过了 Kafka 代理应用程序。 所有这些优化都使 Kafka 能够从接近网络的速度传输消息。 数据分发和复制在这个章节，我们讨论 Kafka 如何实现容错以及它如何在节点之间分配数据。 数据复制分区数据在多个代理（broker）中复制，以便在其中一个代理挂掉时候，依然能够保存数据。在任何时候，一个代理（broker）“拥有”其中一个分区，节点通过这个分区进行读写操作。因此，此分区被称为分区领导者（partition leader）。将接收到的数据复制给其他N个其他代理，称为跟随者（followers）。跟随者同样存储数据，并且随时准备着当 leader 挂掉时，取而代之。这样的配置有助于保证任何成功发布的消息都不会丢失。通过更改复制因子，可以根据数据的重要性来交换性能以获得更强的持久性保证。在其中一个 leader 挂掉时，其他 follower 会竞选上岗，具体算法可以参考：How does a producer/consumer know who the leader of a partition is?作为生产者/消费者，对一个分区进行读取时，首先需要知道对应分区的 leader。这个信息需要存储在可以被访问到的地方，Kafka 使用 Zookeeper 进行存储这些元数据。 何为 ZookeeperZookeeper 是一个分布式键值存储结构。它针对读取进行了高度优化，但写入速度较慢。常应用于存储元数据和处理集群的核心机制（心跳包、分发更新配置等）。它允许服务（Kafka 的 broker）的客户订阅通知，并且能在 Zookeeper 发生变动的时候发送给客户消息。这也是为什么 brokers 能够感知分区的 leader 发生变动。Zookeeper 同时也具有成熟的容错性，或者说，Kafka 很大程度上依赖 Zookeeper 的高容错性。Zookeeper 用于存储所有类型的元数据，包括但不限于： 消费者群组中每个分区的偏移量（尽管现在的客户端在单独的 Kafka 主题 Topic 内存储偏移量） ACL（访问控制列表），用于限制访问/授权 生产者和消费者配额，包括每秒最大信息量 存储分区 leader 和健康状态 如何区分分区的领导者在以往版本中，生产者和消费者经常直接连接并与 Zookeeper 交谈以获取此（和其他）信息。 目前 Kafka 已经弃用这种耦合，从 0.8 和 0.9 版本开始，客户端直接从 Kafka 的 brokers 那里获取元数据信息，他们自己与 Zookeeper 交谈。 流在 Kafka 中，流处理器是从输入的 Topic 中连续读取数据流，并对数据进行一些处理生成数据流以生成主题的任何（或外部服务、数据库、垃圾箱等）内容。对于一些简单的消息，可能使用消费者或生产者的 API 接口直接处理即可，但是涉及到复杂的消息流（例如，多条数据流联合）处理的情况，Kafka 提供一个集成的Stream API库。此 API 应用于自己的代码块中，而不是直接在代理（broker）上运行。它与消费者 API 类似，可以帮助你在多个应用（类似多个消费者）上扩展流处理工作。 无状态处理流的无状态处理是确定性的，不需要依赖任何外部的处理方式。对于任何给定的数据，将始终生成与其他内容无关的相同输入。 流式表的双重性首先要认识到流和表是相同的含义。流，可以解释为表，反之亦然。 流作为表流可以看做对数据进行一系列的更新，因此最终结果作为表进行聚合。这种技术成为事件采集（Event sourcing）。如果你了解如何实现同步数据库的复制，你会知道它是通过所谓的流复制（Streaming replication），每次表格中的变动都会发送到副本服务器。事件采集的另外一个例子是，区块链分类账，它也是进行一系列变化。Kafka 的数据流可以用相同的方式解释，即可以认为是积累到最终的状态的事件。此类流聚合保存在本地 RocksDB 中，称为KTable。 表作为流可以将表视为流中每个键的最新值的快照。 以相同的方式，流记录可以生成表，表更新可以生成更改日志流。 有状态处理一些简单的操作，例如map()或者filter()都是无状态的，不需要额外保存有关处理的任何数据。但是，在现实生活中，大部分操作都是有状态的（例如count()），因此需要保存当前积累的值。假如在流处理器上维护这些状态，流处理器可能会宕机，导致状态丢失。那么应当在哪里保存状态值才能容错呢？一种最简单的方式是简单地将所有状态存储在远程数据库中，并通过网络连接到该数据库。这样做的问题是，没有数据的位置和产生大量的网络交互损耗，这两者都会显着减慢您的应用程序。 一个更微妙但重要的问题是您的流处理作业的正常运行时间将紧密耦合到远程数据库，并且作业将不会自包含（数据库中的数据库与另一个团队的更改可能会破坏您的处理） 。回忆下表和流的二元性。运行我们将流转化为与我们处理位于同一位置的表。它还能提供一种处理容错的机制，即在 Kafka 的 Broker 中存储流。数据流处理器能够在本地表（即，RocksDB）存储状态，该表将从输入流（可能实在某些任意变换之后）更新。当进程失败时，可以通过重新请求流来恢复其数据。你也可以使用一个远程数据库作为流的生产者，用于在本地重建表进行高效的广播更改日志。 KSQL通常，使用 Kafka 只能使用 JVM 语言编写刘处理，因为这是 Kafka 唯一的官方 Streams API 客户端。 2018.04 的 Kafka 发布KSQL，一种可以使用类 SQL 语言来编写简单流媒体工作的工具。通过设置 KSQL 服务器，并且通过 CLI 方式进行交互以此来管理处理。它使用相同的抽象（KStream 和 KTable），保证了 StreamS API 的相同有点（可伸缩性、容错性）和更加简便的方式处理工作流。这个特性虽然不被人经常提起，但经过实践对于测试更有用，甚至运行开发之外的人（例如，产品所有者）使用流处理。 流的可选择性Kafka 的流兼具了力量和简约的完美结合。可是说是市场上处理流工作的最佳工具，与其他流处理工具（Storm、Samza、Spark 和 Wallaroo）相比，Kafka 更容易与其他工具结合。大多数其他流处理的框架的问题在于它们运行和部署的复杂性。例如 Spark 这样的处理框架需要以下几点： 在一组计算机上控制大量的作业，并在集群上有效的分配。 为此，必须动态打包你的程序并将其部署在它需要执行的节点（以及配置、库等）。 为此，要处理以上问题，使得框架尤为复杂。它们需要控制很多方面：部署、配置、监控和打包。Kafka 流能够允许你在你需要时，提出自己的部署策略，例如 Kubernetes、Mesos、Nomad、Docker Swarm 或者其他方式。Kafka Streams 的基本目的是使所有应用程序能够进行流处理，而无需运行和维护另一个操作复杂的集群。 唯一潜在的缺点是它与卡夫卡紧密结合，但在现代世界中，大多数（如果不是全部）实时处理由卡夫卡提供动力可能不是一个很大的劣势。 何时启用 Kafka正如我们已经介绍的，Kafka 允许通过集中式介质获取大量消息并且存储他们，并不担心性能或数据丢失等问题。这意味着非常适合用在系统框架的核心，充当连接不同程序的中间媒介。Kafka 能够成为事件驱动架构的中心部分，是您能够真正的将应用程序间解耦。 Kafka 能够非常轻松的分离不同（微）服务之间的通信。使用 Streams API，现在可以更容易的编写业务逻辑，从而丰富 Kafka 主题数据以便提供服务。 总结Apache Kafka 作为分布式流平台，每天可以处理数以万亿计的事件。Kafka 提供低延迟、高吞吐量、高容错和订阅式流水线，同时能够流式处理事件。我们回顾了它的基本语义（生产者、代理、消费者和 Topic），了解了它的一些优化（page cache），通过复制数据了解了它的容错能力，并且介绍了它不断增长的强大流功能。","categories":[],"tags":[{"name":"珠峰翻译计划","slug":"珠峰翻译计划","permalink":"http://yoursite.com/tags/珠峰翻译计划/"},{"name":"Kafka","slug":"Kafka","permalink":"http://yoursite.com/tags/Kafka/"}]},{"title":"测试语雀-Travis-Hexo","slug":"yuque/测试语雀-Travis-Hexo","date":"2019-01-28T08:16:15.000Z","updated":"2022-02-16T23:07:28.130Z","comments":true,"path":"2019/01/28/yuque/测试语雀-Travis-Hexo/","link":"","permalink":"http://yoursite.com/2019/01/28/yuque/测试语雀-Travis-Hexo/","excerpt":"","text":"信息来源本次语雀到 Travis 的配置，来源于：https://segmentfault.com/a/1190000017797561#articleHeader1 仅分享搭建历程，及代码分析。 TEST 步骤第一次，init 测试。检测是否同步信息，有效期 5 分钟。过期删除第二次测试，时间 5 分钟第三次测试，时间 3 分钟第四次测试，时间 3 分钟，仅测试 POSTMan 触发请求，查看语雀是否会同步至 github。失败第五次测试，测试 yuque-hexo 组件 clean/sync 命令的先后顺序。第六次测试，测试 yuque-hexo 组件，先 clean 后执行 sync 命令。第七次测试，测试 POSTMan 触发请求。第八次测试，测试 serverless 函数的正确性。第九次测试，测试 serverless 函数的正确性。第十次测试，测试 serverless 函数的正确性,参数进行硬编码。第十一次测试，测试 serverless 函数的正确性.第十二次测试，测试 serverless 函数的正确性.第十二次测试，测试 serverless 函数的正确性,测试返回值。第十三次测试，测试 Travis pull queest 的设置是否阻止的触发。第十四次测试，测试 serverless 函数的正确性,移除触发 body 体的 message。第十五次测试，更改上次 body 体的错误，错误传输&lt;‘hexo’&gt;，加了单引号，未识别。成功了，哈哈。。。2019.1.30","categories":[],"tags":[]}]}